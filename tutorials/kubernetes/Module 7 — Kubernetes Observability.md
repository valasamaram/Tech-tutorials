# ğŸ›°ï¸ **Module 7 â€” Kubernetes Observability (Deep Dive)**

**Goal:** Build complete visibility into your cluster using metrics, logs, events, tracing, and dashboards.
Understand HOW Kubernetes works *internally*, diagnose issues, troubleshoot failures, and maintain performance.

---

# ğŸ§­ **Why Observability Matters**

Modern apps are:

* Distributed
* Containerized
* Ephemeral (Pods restart / die frequently)
* Auto-scaling
* Multi-node

This means traditional monitoring **fails**. Kubernetes requires:

* **Metrics** â†’ performance insights
* **Logs** â†’ debugging
* **Tracing** â†’ request flow visualization
* **Events** â†’ cluster-level alerts
* **Dashboards** â†’ SRE visibility
* **Alerts** â†’ proactive detection

Observability answers questions like:

* *Why did Pods restart?*
* *Why is the app slow?*
* *Why is HPA scaling/not scaling?*
* *Why did a node become NotReady?*
* *Why is traffic not reaching the Pod?*

---

# ğŸ”¶ **Section 1 â€” Observability Architecture (The Big Picture)**

Observability in Kubernetes involves **four pillars**:

## ğŸ§© **1. Metrics**

Numerical data: CPU, memory, latency, throughput.
Used for autoscaling, alerting, and dashboards.

## ğŸ§¾ **2. Logs**

Raw application and system output.
Used for debugging and incident resolution.

## ğŸ“£ **3. Events**

Kubernetes internal messages: scheduling failures, evictions, OOMKilled, node issues.

## ğŸ” **4. Traces (Distributed Tracing)**

Shows the path of a request across microservices.

---

# ğŸ“Š **Section 2 â€” Metrics Pipeline (In-Depth)**

**Prometheus** is the de-facto standard.

## ğŸ” How metrics flow:

1. **Kubelets** expose node and Pod metrics
2. **cAdvisor** exposes container metrics
3. **kube-state-metrics** exposes cluster state (deployments, jobs, replicas)
4. **Prometheus server** scrapes all endpoints
5. **Grafana** visualizes metrics
6. Optional: **Alertmanager** sends alerts (Slack, PagerDuty, Email)

### ğŸ”¹ Key Metric Types:

| Type      | Example                      |
| --------- | ---------------------------- |
| Counter   | total requests served        |
| Gauge     | CPU usage at a moment        |
| Histogram | request latency distribution |
| Summary   | percentiles (p90, p95, p99)  |

### ğŸ”¹ Critical Kubernetes Metrics to Track:

#### **Pods**

* Restarts
* CPU throttling
* Memory usage vs limits
* Pending Pods

#### **Nodes**

* CPU pressure / memory pressure
* Disk IO bottlenecks
* Network saturation
* Node Ready/NotReady status

#### **Cluster**

* HPA behavior
* Evictions
* Auto-scaler activity
* API server latency

Example: CPU usage metric

```
container_cpu_usage_seconds_total
```

---

# ğŸ“„ **Section 3 â€” Logging Architecture (Deep Dive)**

Logs in Kubernetes are scattered across:

* Containers
* Nodes
* Kubelet
* Network plugin
* API Server

### ğŸ”§ Logging Pipeline Components:

1. **Fluentd / Fluent Bit / Vector / Logstash**
   Collect logs from nodes & containers.

2. **Elasticsearch / OpenSearch / Loki**
   Stores logs.

3. **Grafana / Kibana**
   Visualizes log data.

### ğŸ”¹ Types of Logs:

| Type             | Description                    |
| ---------------- | ------------------------------ |
| Application logs | app stderr/stdout              |
| Pod logs         | via `kubectl logs`             |
| Node logs        | OS & Kubelet logs              |
| Cluster logs     | API server, controller-manager |
| Audit logs       | security-focused logging       |

### ğŸ”¹ Common Log Issues:

* â€œCrashLoopBackOffâ€
* â€œOOMKilledâ€
* ImagePullBackOff
* Node disk pressure
* CNI failures

---

# ğŸ† **Section 4 â€” Kubernetes Events (Instant Troubleshooting)**

Events reveal:

* **Pod scheduling failures**
* **Node problems**
* **Evictions**
* **Image pull issues**
* **Volume attach/detach failures**

Commands:

```
kubectl get events --sort-by='.metadata.creationTimestamp'
kubectl describe pod <pod>
```

Example events:

* `FailedScheduling`
* `BackOff`
* `Unhealthy`
* `FailedMount`
* `NodeNotReady`

Events are ephemeral â†’ use:

* Eventrouter
* Loki
* Kubernetes dashboard

---

# ğŸ§µ **Section 5 â€” Distributed Tracing (Advanced Production Setup)**

Tracing shows **how a request flows across services**.

Tools:

* **Jaeger**
* **OpenTelemetry**
* **Zipkin**

### ğŸ” Why Tracing Is Critical:

* Debug microservice performance
* Identify bottlenecks
* Measure latency breakdown
* Detect slow downstream dependencies

### Trace Example:

```
Frontend â†’ Auth Service â†’ Product Service â†’ Database
```

Tracing reveals:

* Slow queries
* Network hops
* Serialization overhead

---

# ğŸ–¥ **Section 6 â€” Dashboards (What SREs Use Daily)**

Grafana dashboards provide full visibility.

### Must-Have Dashboards:

#### 1ï¸âƒ£ **Node Health Dashboard**

* CPU, memory, disk IOPS
* Node pressure conditions

#### 2ï¸âƒ£ **Pod/Deployment Dashboard**

* Pod restarts
* Replica count
* Resource usage

#### 3ï¸âƒ£ **HPA Dashboard**

* Target vs actual CPU
* Scaling events

#### 4ï¸âƒ£ **Network Dashboard**

* Latency
* Dropped packets
* DNS failures

#### 5ï¸âƒ£ **Storage Dashboard**

* PVC usage
* Latency (I/O)
* Volume attach delays

---

# ğŸ”” **Section 7 â€” Alerting (Proactive Monitoring)**

Using **Alertmanager** or cloud-native equivalents.

### ğŸ”¥ Critical Alerts for Production:

#### Nodes:

* NodeNotReady
* DiskPressure
* MemoryPressure

#### Pods:

* CrashLoopBackOff
* Image pull failure
* High restart count

#### Apps:

* Error rate > threshold
* Latency (p95/p99) high
* DB connection failure

Triggers:

* Slack
* PagerDuty
* OpsGenie
* Email

---

# ğŸ›  **Section 8 â€” Tooling Ecosystem**

### Core:

* **Prometheus**
* **Grafana**
* **Alertmanager**

### Logs:

* Fluentd / Fluent Bit
* Loki
* Elasticsearch

### Tracing:

* Jaeger
* OpenTelemetry
* Zipkin

### Hosted alternatives:

* Datadog
* New Relic
* Dynatrace
* Elastic Cloud
* AWS CloudWatch
* Azure Monitor
* GCP Ops Suite

---

# ğŸ§  **Section 9 â€” Advanced Debugging & Triage Techniques**

### ğŸ§© Pod level:

```
kubectl describe pod
kubectl logs <pod>
kubectl exec -it <pod> -- bash
```

### ğŸ§© Node level:

```
kubectl describe node
journalctl -u kubelet
dmesg
```

### ğŸ§© Network level:

* Check CNI plugin
* Validate Services/Endpoints
* Test DNS with:

```
kubectl exec -it <pod> -- nslookup service-name
```

### ğŸ§© Storage level:

* Check PVC events
* Look for `FailedMount` errors

---

# ğŸ”’ **Section 10 â€” Observability for Security**

Capture:

* Audit logs
* Unauthorized access attempts
* API server logs
* Pod exec logs
* Network flow logs

Requirements for Zero Trust:

* Tracing
* Correlated logs
* Full request metadata

---

# ğŸ” **Section 11 â€” Observability for Autoscaling**

Autoscalers depend on observability:

### HPA needs:

* CPU metrics
* Memory metrics
* Custom metrics

### VPA needs:

* historical resource usage

### Cluster Autoscaler needs:

* Pending Pod metrics
* Node utilization

---

# ğŸ **After Module 7 You Will Be Able To:**

âœ” Build a complete observability stack
âœ” Configure metrics, logs, tracing, events
âœ” Build dashboards for SRE teams
âœ” Troubleshoot Pod/Node/Network/Storage issues
âœ” Create alerts for incidents
âœ” Integrate observability with autoscaling
âœ” Monitor microservices end-to-end
âœ” Use Prometheus, Grafana, FluentBit, Loki, Jaeger
âœ” Achieve production-grade monitoring

---

