# ğŸš€ **Module 6 â€” Autoscaling & Reliability**

*How Kubernetes keeps your apps fast, stable, and cost-efficient â€” automatically.*

This module teaches everything about **HPA, VPA, Cluster Autoscaler, Pod disruption rules, probes, affinities, PDBs, and high availability**.

---

# ğŸ§­ **Section 1 â€” The Philosophy of Reliability in Kubernetes**

Kubernetes reliability is based on 3 principles:

### 1ï¸âƒ£ **Self-Healing**

If a container or Pod crashes â†’ Kubernetes restarts it.
If a node dies â†’ Pods rescheduled automatically.

### 2ï¸âƒ£ **Elasticity**

Scale apps up/down based on demand using autoscalers.

### 3ï¸âƒ£ **Fault Tolerance**

Distribute workloads so loss of a node or zone doesnâ€™t impact availability.

---

# âš™ï¸ **Section 2 â€” Liveness & Readiness Probes (Foundations of Reliable Pods)**

Probes determine whether a container is healthy.

### ğŸ”¥ Liveness Probe

Checks **if app is alive**.
If it fails â†’ container is restarted.

Example:

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
```

### ğŸšª Readiness Probe

Checks **if app is ready to serve traffic**.
If it fails â†’ Pod removed from service endpoints.

Example:

```yaml
readinessProbe:
  exec:
    command: ["cat", "/tmp/ready"]
```

### ğŸ§Š Startup Probe

Used for slow-start apps (Java Spring Boot, ML models, etc.).

```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```

---

# ğŸ“ˆ **Section 3 â€” Horizontal Pod Autoscaler (HPA)**

Scales the **number of Pods**.

### ğŸ” How HPA Works:

* Reads metrics (CPU, memory, custom metrics)
* Compares metrics vs target
* Scales up/down replicas accordingly

Example:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
```

### Supported Metrics

| Type             | Example                           |
| ---------------- | --------------------------------- |
| CPU              | 70% utilization                   |
| Memory           | 80% utilization                   |
| Custom Metrics   | requests per second, queue length |
| External Metrics | SQS queue size, Kafka lag         |

---

# ğŸ“¦ **Section 4 â€” Vertical Pod Autoscaler (VPA)**

Adjusts **CPU/memory limits** automatically.

Useful for:

* ML workloads
* Batch jobs
* Pods with unpredictable compute needs

Three modes:

1. **Off** â†’ only recommendations
2. **Auto** â†’ apply changes live
3. **Initial** â†’ apply on Pod creation only

Example:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  updatePolicy:
    updateMode: "Auto"
```

âš ï¸ **Note:**
HPA + VPA can conflict unless carefully configured.

---

# ğŸ— **Section 5 â€” Cluster Autoscaler (Node Autoscaling)**

Adds/removes **worker nodes** automatically.

### When nodes scale **UP**

* Pods are pending (insufficient CPU/memory)

### When nodes scale **DOWN**

* Node has < 50% utilization
* All Pods can be rescheduled elsewhere

### Supported Platforms

* AWS EKS
* Azure AKS
* Google GKE
* Karpenter (AWS) â€” modern alternative

---

# ğŸ§© **Section 6 â€” PodDisruptionBudgets (PDB) for High Availability**

PDB ensures a minimum number of Pods stay running during:

* Node upgrades
* Drains
* Autoscaler scale-down
* Evictions

Example: Keep 2 replicas always available

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api
```

---

# ğŸ§­ **Section 7 â€” Affinity, NodeSelectors & Taints/Tolerations**

These control **where Pods run**, improving reliability and performance.

---

## ğŸ§² **Node Affinity**

Schedule Pod to specific nodes (e.g., GPU nodes).

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: gpu
              operator: In
              values:
                - "true"
```

---

## ğŸ“ **Pod Affinity & Anti-Affinity**

Goal: Spread Pods across nodes/zones.

### ğŸ§© Anti-Affinity Example

Prevent all Pods of same app from being on same node (HA):

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: backend
        topologyKey: "kubernetes.io/hostname"
```

This ensures:

* Each Pod on a **different node**
* Improved fault tolerance

---

## ğŸš« Taints & Tolerations

Nodes can â€œrepelâ€ Pods unless they tolerate the taint.

Example:
Node reserved for DB workloads:

```bash
kubectl taint nodes db-node workload=db:NoSchedule
```

Pod must tolerate it:

```yaml
tolerations:
  - key: workload
    operator: Equal
    value: db
    effect: NoSchedule
```

---

# ğŸ’¥ **Section 8 â€” Multi-Zone & Multi-Region HA**

Used by production companies (Netflix, Uber, Salesforce).

### Best Practices:

* Spread nodes across zones
* Use topology-aware routing
* Use anti-affinity for Pods
* Use regional load balancers
* Store data in multi-AZ persistent storage (EBS, Managed Disks, PD)

---

# ğŸ”„ **Section 9 â€” Rolling Updates & Rollbacks**

Deployment ensures:

* Zero downtime releases
* Gradual rollouts
* Automatic rollback on failure

Example configuration:

```yaml
strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
```

### Canary or Blue-Green deployment

Use tools:

* Argo Rollouts
* Flagger
* Istio traffic shifting

---

# ğŸ©º **Section 10 â€” Self-Healing Mechanisms**

Kubernetes automatically:

* Restarts crashed containers
* Reschedules Pods when node fails
* Recreates Pods if deleted
* Repairs replicas to maintain desired state

---

# ğŸ¯ **Section 11 â€” Reliability Patterns**

### âœ” Circuit breakers

Via service mesh (Istio/Linkerd)

### âœ” Timeouts & retries

Configured via mesh or app settings

### âœ” Bulkheads

Isolate workloads across nodes/zones

### âœ” Rate limiting

Protect backend services

### âœ” Auto restart policies

`OnFailure`, `Always`, `Never`

---

# ğŸ **After Module 6 You Will Be Able To:**

âœ” Enable auto-scaling for both Pods and nodes
âœ” Properly configure cluster self-healing
âœ” Ensure zero-downtime deployments
âœ” Build Fault-Tolerant applications
âœ” Protect your app from overloads
âœ” Configure high availability across nodes & zones
âœ” Use PDBs, probes, affinities, taints effectively
âœ” Build reliable, enterprise-grade platforms

---

